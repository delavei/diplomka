sudo ./configure --enable-docker --enable-virt --enable-hadoop --enable-write-tsdb --enable-logfile --disable-all-plugins
sudo /opt/collectd/sbin/collectd -f
python diplomka/kod/tsddrain.py 4242 opentsdb/
sudo docker run -it debian  /bin/bash
sudo docker run -it ubuntu  /bin/bash

OTAZKYYYYY
analyza a navrhy - metriky o konfigurácií zberať menej často - spominam to v kapitole metriky
jake verzie softov sa používajú? dať to do textu -- libvirt centOS7, debian 7,8
zberat z dockeru pre jednotlivé jadrá?
hadoop kontajnery?
hadoop node state zberať?
pamatove statsitiky libvirt, iba niektore ide zberat, ostatne mam vyhodit?
vyhodil som z hadoopu aj z inych zoznamov metrik to, co nema moc vyznam zberat, nie su tam komplet podrobne vypisy, co to poskytuje - to asi skor len do prilohy

TEEEEEEEEEEEEEEEEEEEXT
Niektoré metriky sa v čase menia menej ako iné, napr. počet odoslaných chybných bajtov sa na sieťovom rozhraní nemení tak rýchlo ako počet odoslaných bajtov. V prípade, že
by teda bol prírastok nulový, bolo by možné metriku neodoslať   a uštriť záťaž? toto iba napísať, rieiši to opentsdb
Možný problém - dotazovanie na metriky v rôznych intervaloch, ale využívanie jedného pripojenia  - je to v metriky - periodicita zbierania metrík - riešiť toto?

okomentovať, že nie všetky poskytované metriky má význam aj zberať

napisat o tom, ako sa vypocitava cpu load - podporit citaciou, ako sa to vlastne vyratava - napriklad aj ze zo zdrojaku

tabulka - nazvy metrik tak ako ich ukladam do databazy vs nazvy ako ich dostavam z vystupov 

v analyze a navrhu spomenúť že zberám aj metadáta




POZNAMKYYYYYYYYYYYYYYYYYYYYYY
obrazky - hadoop, metacentrum
cgroup - v gride server distribuuje ulohy, obali ich do  cgroup na konkretnom stroji a obcas sa tam pozrie, kolko zozerie

citacie - co je cloud, zen, iaas, paas, co je vytazenie zdrojov - ganglia bola akademická, najlepsie nieco tlacene. casova rada, tie metody nejake nejasne o tom.

oddelit monitorovanie a uctovanie - napr. detekcia zdrojov a uzivatelov - ak su vhodne zbierane metriky a metedata, da sa to vyuzit na accounting. mozem uvadzat misovu diplomku

brendan gregg linuxperf

analyza a navrh - vysoky monitorovaci vykon, skalovatelnost - mozno mat cisla, porovnat to s niecim a presne na com som to testoval
				a vsetko o merani - napr. tcollector






IMPLEMENTACIA
dorobit configure skript, aby sa kopiroval custom types

docker
robit pravidelne refresh zoznamu kontajnerov
config - refresh interval pre kontajnery

hadoop
adresa klastra do konfiguracie
mena appiek, kde su konrolne vyrazy \n\t a tak sposobuju chaos v tsd. ked je meno appky nakonci, je to pohode
dorobiť cluster a node metriky
















	
	monitorovacie softy
	\section{Cacti}
Cacti is a complete network graphing solution designed to harness the power of RRDTool's data storage and graphing functionality. Cacti provides a fast poller, advanced graph templating, multiple data acquisition methods, and user management features out of the box.
\cite{11}

\subsection{Ostatne} 
\begin{description}
\item[\emph{Zenoss}]
\item[\emph{Munin}]
\end{description}

\section{Scout}
Scout runs within Docker containers without any special configuration. \cite{scout}

\section{New Relic}
http://newrelic.com/docker



\section{Reakcia na dlhú odozvu modulu}
Ak je hodnota nejakej metriky mimo určitý rozsah, je generované hlásenie. Na to je možné reagovať.
Na situáciu, keď časť zodpovedná za zbieranie dát neodpovedá, je ale možné reagovať len reštartovaním celej monitorovacej aplikácie. Nie je možné jednotlivé pluginy ovládať nezávisle. 

V princípe nejde nijako odlíšiť, či daný modul čaká na údaje alebo došlo k chybe a modul neodpovedá. Preto bude potrebné vytvoriť niektoré pluginy tak, aby jedna časť bola neustále dostupná 
a reagovala na výzvy od riadiacej aplikácie. Bude definovaný časový interval na vrátenie hodnoty. V prípade ak plugin úspešne v časovom intervale zistil dané metrické dáta, vráti ich riadiacej aplikácií. V prípade, že v danom intervale plugin neobdržal metrické dáta,
vráti poslednú hodnotu. Zároveň sa nebudú vytvárať nové požiadavky na tento údaj. Tento časový interval si bude môcť užívateľ nastaviť pre každú sondu. Predmetom testovania bude zistiť, 
aký interval by bol vhodný pre tú ktorú sondu.

Ďalšou prahovou hodnotou bude počet opakovaní, pri ktorých plugin vracia poslednú hodnotu danej metriky. Ak dôjde k prekročeniu tejto hodnoty, bude reštartovaná celá monitorovacia aplikácia.


\subsection{Centrálny systém notifikácií a manažmentu zberných modulov}
Vyššie popísané riešenie predstavuje decentralizovanú architektúru. Každá zo sond funguje nezávisle, čo sťažuje prípadné mechanizmy konfigurácie
a riešenie krízových situácií ako napr. zastavenie činnosti démona. Toto je jeden z aspektov kde má navrch napr. systém Zabbix. Súčasťou
môjho riešenia bude zároveň aplikácia, ktorá bude spravovať konfigurácie jednotlivých inštancií collectd, umožní ich meniť a podľa toho ich reštartovať.
Umožní prehliadať notifikácie z jednotlivých sond a manuálne ovládať inštancie a ich moduly. To bude zároveň vyžadovať modifikáciu démona
collectd, ktorý v súčasnosti neumožňuje reštartovanie jednotlivých modulov.
\\Takýto systém konfigurácie môže mať význam pre lepšiu distribúciu monitorovacej záťaže, kedy skupina démonov môže odosielať dáta jednému 
démonovi časovej rady a iná skupina bude využívať iného časového démona.
